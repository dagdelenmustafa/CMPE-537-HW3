# -*- coding: utf-8 -*-
"""ImageClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11yqlQuss80jB66JPqBfPHHvo0Hwp8nuq

#Image Classifier using Local Descriptors

##Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import cv2

import pickle

from imutils import paths
from collections import Counter

from imblearn.under_sampling import NearMiss
from imblearn.under_sampling import CondensedNearestNeighbour
from imblearn.under_sampling import TomekLinks
from imblearn.under_sampling import EditedNearestNeighbours
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler 

from skimage import feature
from skimage import exposure

import sklearn
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import AgglomerativeClustering
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve, confusion_matrix

from google.colab import drive

# !pip3 freeze > requirements.txt

drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/Colab Notebooks/Image Classification/Caltech20/'

training_dir = root_path + 'training/'
test_dir = root_path + 'testing/'

# !unzip 'gdrive/My Drive/Colab Notebooks/Image Classification/Caltech20.zip' -d 'gdrive/My Drive/Colab Notebooks/Image Classification/'

"""##Functions and Parameters"""

classes = ['airplanes',
 'anchor',
 'background_class',
 'barrel',
 'camera',
 'car_side',
 'dalmatian',
 'Faces',
 'ferry',
 'headphone',
 'lamp',
 'pizza',
 'pyramid',
 'snoopy',
 'soccer_ball',
 'stop_sign',
 'strawberry',
 'sunflower',
 'water_lilly',
 'windsor_chair',
 'yin_yang']

def dataset_prep(dir_path, classes):
  
  labels = []
  flag = 0

  for imagePath in paths.list_images(dir_path):
    
    image_class = imagePath.split("/")[-2]

    img = Image.open(imagePath).convert('L') # To open image as gray scale
    image_array = np.asarray(img) 

    #####
    # resized = cv2.resize(image_array, (128,128))
    # im_opencv = Image.fromarray(resized)
    # H = feature.hog(im_opencv, orientations=9, 
    #                 pixels_per_cell=(8, 8), 
    #                 cells_per_block=(2, 2), 
    #                 transform_sqrt=False, 
    #                 block_norm="L2")
    # H = H.reshape([225,72])
    #####
    H = hog_calculator(image_array)

    i = classes.index(image_class) + 1
    labels.append(i)

    if flag == 0:
      flag = 1
      descriptors = H.copy()
    else:
      descriptors = np.concatenate((descriptors, H), axis=0) 

  return labels, descriptors

"""###HOG Descriptor"""

def hog_calculator(image_array, block_size = (2, 2), cell_size = (8, 8), s=(128, 128), orientation=9):

  eps = 1e-5 # For normalization

  local_descriptors = []

  n = int(s[0]//cell_size[0]) # number of cells in a row
  m = int(s[1]//cell_size[1]) # number of cells in a column

  block_n = n+1-block_size[0] # Number of blocks per row
  block_m = m+1-block_size[1] # Number of blocks per column

  # First step is to resize the image into 128x256 size
  resized_image = cv2.resize(image_array, s)
  im = Image.fromarray(resized_image)

  histogram_matrix = np.zeros((n, m, orientation))

  for j in range(m):
    for i in range(n):

      # Boundaries of a cell
      cell_x_0 = i*cell_size[0]
      cell_x_1 = (i+1)*cell_size[0]
      cell_y_0 = j*cell_size[1]
      cell_y_1 = (j+1)*cell_size[1]

      cell = resized_image[cell_y_0:cell_y_1, cell_x_0:cell_x_1]
      magnitudes = np.zeros(cell.shape)
      orients = np.zeros(cell.shape)

      cell = cell.astype(np.int32)
      cell_row, cell_col = cell.shape

      y_grad = np.zeros(cell.shape, dtype=np.double)
      x_grad = np.zeros(cell.shape, dtype=np.double)

      y_grad[1:-1,:] = cell[2:, :] - cell[:-2, :]
      x_grad[:,1:-1] = cell[:,2:] - cell[:,:-2]

      magnitudes = np.sqrt(x_grad**2 + y_grad**2)
      orients = np.degrees(np.arctan2(y_grad, x_grad)) % 180 # According to the paper

      bins = np.arange(10, 190, 20) #If orientation != 9 change this 
      hist = np.zeros(bins.shape)

      # Turning gradients and orientations into histogram
      for row in range(cell_row):
        for col in range(cell_col):

          angle = orients[row,col]
          mag = magnitudes[row,col]

          position = np.argmax(bins > angle) % 9
          hist[position-1] += (((bins[position] - angle)%180)/20)*mag
          hist[position] += (((angle - bins[position-1])%180)/20)*mag

      histogram_matrix[i, j] = hist

  for i in range(block_n): 
    for j in range(block_m): 
      block_array = np.concatenate((histogram_matrix[i, j],histogram_matrix[i+1, j], histogram_matrix[i, j+1], histogram_matrix[i+1, j+1]))

      #L2 Normalization 
      norm = np.sqrt(np.sum(block_array ** 2) + eps ** 2) 
      block_array = block_array/norm
    
      local_descriptors.append(block_array)

  # H = np.array(local_descriptors)
  H = np.array(local_descriptors).flatten()
  H_norm = np.sqrt(np.sum(H ** 2) + eps ** 2)
  H = H/H_norm
  H = H.reshape([block_n*block_m,36])

  return H

"""###HOG Visualization"""

# imgPath = training_dir + 'airplanes/image_0002.jpg'
# img = Image.open(imgPath).convert('L')
# image_array = np.asarray(img)

# resized = cv2.resize(image_array, (128,128))
# im_opencv = Image.fromarray(resized)
# H = feature.hog(im_opencv, orientations=9, 
#                 pixels_per_cell=(8, 8), 
#                 cells_per_block=(2, 2), 
#                 transform_sqrt=False, 
#                 block_norm="L2")

# H2 = hog_calculator(image_array)
# H = H.reshape([225,36])
# distance = np.linalg.norm((H - H2), axis=1)
# print(distance)

"""###Dictionary - Hierarchical Clustering & K-means"""

def hierarchical_clustering(descriptors, cluster_size):
  cluster = AgglomerativeClustering(n_clusters=cluster_size, affinity='euclidean', linkage='ward')
  cluster.fit_predict(descriptors)
  return cluster.labels_

def kmeans_clustering(descriptors, cluster_size, seed):
  kmeans = KMeans(n_clusters=cluster_size, max_iter=100,  random_state=seed)
  kmeans.fit(descriptors)
  return kmeans, kmeans.labels_

"""###Quantization"""

def bag_of_visual_words(clusters, cluster_size, descriptor_size):

  X = []
  n = clusters.shape[0]
  image_number = int(n/descriptor_size)

  for i in range(image_number):
    hist = np.zeros(cluster_size)
    clusters_i = clusters[i*descriptor_size:(i+1)*descriptor_size] 

    for j in range(cluster_size):
      hist[j]  = np.count_nonzero(clusters_i == j)
      
    hist = hist/descriptor_size
    X.append(hist)

  return X

"""###Parameters"""

seed = 1234
cluster_size = 50
image_resize = (128, 128)
descriptor_size = int((image_resize[0]/8-1)*(image_resize[1]/8-1))

"""##Dataset Prep

###Training Data
"""

labels, descriptors = dataset_prep(training_dir, classes)

kmeans, clusters = kmeans_clustering(descriptors, cluster_size, seed)

X = bag_of_visual_words(clusters, cluster_size, descriptor_size)
y_train = labels

counter = Counter(labels)
print(counter)

under_str = {1:400, 3:400, 5: 400, 6:400, 7:400, 9:400, 14:400, 15:400, 17:400, 19:400}
# over_str = {2:500, 4:500, 10:500, 12:400}

# oversample = RandomOverSampler(sampling_strategy=over_str)
# X_over, y_over = oversample.fit_resample(X, labels)

# undersample = NearMiss(version=1, n_neighbors=3)
# undersample = CondensedNearestNeighbour(n_neighbors=3)
# undersample = TomekLinks(sampling_strategy='majority')
undersample = RandomUnderSampler(sampling_strategy=under_str)

X_train, y_train = undersample.fit_resample(X, labels)

counter_undersampled = Counter(y_train)
for i in range(1,22):
  print(i, classes[i-1], '\t',counter_undersampled[i])

s = set(counter_undersampled)
print(s)

"""###Test Data"""

y_test , X_test = dataset_prep(test_dir, classes)

test_clusters = kmeans.predict(X_test)

X_test = bag_of_visual_words(test_clusters, cluster_size, descriptor_size)

"""##Classifier - Random Forest"""

modelRF = RandomForestClassifier(criterion='gini', 
                                 oob_score=True, 
                                 random_state=seed, 
                                 bootstrap= True, 
                                 max_depth=50, 
                                 max_features='auto', 
                                 min_samples_leaf= 1, 
                                 min_samples_split=5, 
                                 n_estimators=800)
modelRF.fit(X_train, y_train)

print(modelRF.score(X_test, y_test))
print(modelRF.oob_score_)

parameters = {'criterion' : ['gini'],
              'bootstrap': [True],
              'max_depth': [10, 20, 40, 80, None],
              'max_features': ['auto', 'sqrt'],
              'min_samples_leaf': [1, 2, 4],
              'min_samples_split': [2, 5, 10],
              'n_estimators': [100, 200, 400]}

# RF = RandomForestClassifier(random_state=seed)

# gridsearch = GridSearchCV(RF, parameters, refit='AUC')
# gridsearch.fit(X_train, y_train)

# parameters = {'criterion' : ['gini'],
#               'bootstrap': [True],
#               'max_depth': [5, 10, 50, 100],
#               'n_estimators': [400, 600, 800]}

# RF = RandomForestClassifier(random_state=seed)

# gridsearch = GridSearchCV(RF, parameters, refit='f1_macro')
# gridsearch.fit(X_train, y_train)

# gridsearch.best_params_

"""###Performance Evaluation - RF"""

RF_predictions = modelRF.predict(X_test)
hmap = sns.heatmap(metrics.confusion_matrix(y_test, RF_predictions) , cmap='Blues', xticklabels=classes, yticklabels=classes)

print(classification_report(y_test, RF_predictions, target_names=classes))

"""##MLP"""

mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)

mlp.score(X_test, y_test)

MLP_predictions = mlp.predict(X_test)
hmap = sns.heatmap(metrics.confusion_matrix(y_test, MLP_predictions) , cmap='Blues', xticklabels=classes, yticklabels=classes)

print(classification_report(y_test, MLP_predictions, target_names=classes))