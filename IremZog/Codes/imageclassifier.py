# -*- coding: utf-8 -*-
"""ImageClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11yqlQuss80jB66JPqBfPHHvo0Hwp8nuq

#Image Classifier using Local Descriptors

##Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import cv2

import pickle

from imutils import paths
from collections import Counter

from imblearn.under_sampling import NearMiss
from imblearn.under_sampling import CondensedNearestNeighbour
from imblearn.under_sampling import TomekLinks
from imblearn.under_sampling import EditedNearestNeighbours
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler 

from skimage import feature
from skimage import exposure

from sklearn.model_selection import GridSearchCV
from sklearn.cluster import AgglomerativeClustering
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve, confusion_matrix

from google.colab import drive

drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/Colab Notebooks/Image Classification/Caltech20/'

training_dir = root_path + 'training/'
test_dir = root_path + 'testing/'

# !unzip 'gdrive/My Drive/Colab Notebooks/Image Classification/Caltech20.zip' -d 'gdrive/My Drive/Colab Notebooks/Image Classification/'

"""##Functions and Parameters"""

classes = ['airplanes',
 'anchor',
 'background_class',
 'barrel',
 'camera',
 'car_side',
 'dalmatian',
 'Faces',
 'ferry',
 'headphone',
 'lamp',
 'pizza',
 'pyramid',
 'snoopy',
 'soccer_ball',
 'stop_sign',
 'strawberry',
 'sunflower',
 'water_lilly',
 'windsor_chair',
 'yin_yang']

def dataset_prep(dir_path, classes):
  
  labels = []
  flag = 0

  for imagePath in paths.list_images(dir_path):
    
    image_class = imagePath.split("/")[-2]

    img = Image.open(imagePath).convert('L') # To open image as gray scale
    image_array = np.asarray(img) 

    H = hog_calculator(image_array)

    i = classes.index(image_class) + 1
    labels.append(i)

    if flag == 0:
      flag = 1
      descriptors = H.copy()
    else:
      descriptors = np.concatenate((descriptors, H), axis=0) 

  return labels, descriptors

"""###HOG Descriptor"""

def hog_calculator(image_array, block_size = (2, 2), cell_size = (8, 8), s=(64, 128)):

  local_descriptors = []

  n = int(s[0]/cell_size[0]) # number of cells in a row
  m = int(s[1]/cell_size[1]) # number of cells in a column

  # First step is to resize the image into 128x256 size
  resized_image = cv2.resize(image_array, s)
  im = Image.fromarray(resized_image)

  histogram_matrix = np.zeros((n, m, 18))

  for j in range(m):
    for i in range(n):

      # Boundaries of a cell
      cell_x_0 = i*cell_size[0]
      cell_x_1 = (i+1)*cell_size[0]
      cell_y_0 = j*cell_size[1]
      cell_y_1 = (j+1)*cell_size[1]

      cell = resized_image[cell_y_0:cell_y_1, cell_x_0:cell_x_1]
      magnitudes = np.zeros(cell.shape)
      orients = np.zeros(cell.shape)

      cell = cell.astype(np.int32)
      cell_row, cell_col = cell.shape

      cell = np.pad(cell, 1 , mode='constant')

      # Compute gradients and orientations
      for k in range(1, cell_row+1):
        for l in range(1, cell_col+1):

          y_grad = cell[k+1,l] - cell[k-1, l]
          x_grad = cell[k, l+1] - cell[k, l-1]
          magnitudes[k-1,l-1] = np.sqrt(x_grad**2 + y_grad**2)
          orients[k-1,l-1] = np.degrees(np.arctan2(y_grad, x_grad)) + 180

      bins = np.arange(0, 360, 20) 
      hist = np.zeros(bins.shape)

      # Turning gradients and orientations into histogram
      for row in range(cell_row):
        for col in range(cell_col):

          angle = orients[row,col]
          mag = magnitudes[row,col]

          position = np.argmax(bins > angle)

          hist[position-1] += ((bins[position] - angle)/20)*mag
          hist[position] += ((angle - bins[position-1])/20)*mag

      histogram_matrix[i, j] = hist

  for i in range(n+1-block_size[0]): # Number of blocks per row
    for j in range(m+1-block_size[1]): # Number of blocks per column
      block_array = np.concatenate((histogram_matrix[i, j],histogram_matrix[i+1, j], histogram_matrix[i, j+1], histogram_matrix[i+1, j+1]))
      norm = np.linalg.norm(block_array)
      if norm != 0:
        block_array = block_array/norm
      local_descriptors.append(block_array)

  return np.array(local_descriptors)

# imagePath = training_dir + 'strawberry/image_0130.jpg'
  # img = Image.open(imagePath).convert('L')
  # image_array = np.asarray(img) 
  # print(image_array.shape)
  # local_descriptors = hog_calculator(image_array)

# local_descriptors.shape

# plt.imshow(img)
# plt.show()

"""###HOG Visualization"""

# H, hogImage = feature.hog(logo, orientations=9, 
#                 pixels_per_cell=(8, 8), 
#                 cells_per_block=(2, 2), 
#                 transform_sqrt=True, 
#                 block_norm="L1",
#                 visualize=True)

# hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))
# hogImage = hogImage.astype("uint8")

# plt.imshow(hogImage)
# plt.show()

"""###Dictionary - Hierarchical Clustering & K-means"""

def hierarchical_clustering(descriptors, cluster_size):
  cluster = AgglomerativeClustering(n_clusters=cluster_size, affinity='euclidean', linkage='ward')
  cluster.fit_predict(descriptors)
  return cluster.labels_

def kmeans_clustering(descriptors, cluster_size, seed):
  kmeans = KMeans(n_clusters=cluster_size, max_iter=100,  random_state=seed)
  kmeans.fit(descriptors)
  return kmeans, kmeans.labels_

"""###Quantization"""

def bag_of_visual_words(clusters, cluster_size, descriptor_size):

  X = []
  n = clusters.shape[0]
  image_number = int(n/descriptor_size)

  for i in range(image_number):
    hist = np.zeros(cluster_size)
    clusters_i = clusters[i*descriptor_size:(i+1)*descriptor_size] 

    for j in range(cluster_size):
      hist[j]  = np.count_nonzero(clusters_i == j)
      
    hist = hist/descriptor_size
    X.append(hist)

  return X

"""###Parameters"""

seed = 1234
cluster_size = 50
image_resize = (64, 128)
descriptor_size = int((image_resize[0]/8-1)*(image_resize[1]/8-1))

"""##Dataset Prep

###Training Data
"""

labels, descriptors = dataset_prep(training_dir, classes)

kmeans, clusters = kmeans_clustering(descriptors, cluster_size, seed)

X = bag_of_visual_words(clusters, cluster_size, descriptor_size)
y_train = labels

counter = Counter(labels)
print(counter)

under_str = {1:400, 3:400, 5: 400, 6:400, 7:400, 9:400, 14:400, 15:400, 17:400, 19:400}
# over_str = {2:500, 4:500, 10:500, 12:400}

# oversample = RandomOverSampler(sampling_strategy=over_str)
# X_over, y_over = oversample.fit_resample(X, labels)

# undersample = NearMiss(version=1, n_neighbors=3)
# undersample = CondensedNearestNeighbour(n_neighbors=3)
# undersample = TomekLinks(sampling_strategy='majority')
undersample = RandomUnderSampler(sampling_strategy=under_str)

X_train, y_train = undersample.fit_resample(X, labels)

counter_undersampled = Counter(y_train)
for i in range(1,22):
  print(i, classes[i-1], '\t',counter_undersampled[i])

s = set(counter_undersampled)
print(s)

"""###Test Data"""

y_test , X_test = dataset_prep(test_dir, classes)

test_clusters = kmeans.predict(X_test)

X_test = bag_of_visual_words(test_clusters, cluster_size, descriptor_size)

"""##Classifier - Random Forest"""

modelRF = RandomForestClassifier(criterion='gini', 
                                 oob_score=True, 
                                 random_state=seed, 
                                 bootstrap= True, 
                                 max_depth=50, 
                                 max_features='auto', 
                                 min_samples_leaf= 1, 
                                 min_samples_split=5, 
                                 n_estimators=800)
modelRF.fit(X_train, y_train)

print(modelRF.score(X_test, y_test))
print(modelRF.oob_score_)

# parameters = {'criterion' : ['gini'],
#               'bootstrap': [True],
#               'max_depth': [10, 20, 40, 80, None],
#               'max_features': ['auto', 'sqrt'],
#               'min_samples_leaf': [1, 2, 4],
#               'min_samples_split': [2, 5, 10],
#               'n_estimators': [100, 200, 400]}

# RF = RandomForestClassifier(random_state=seed)

# gridsearch = GridSearchCV(RF, parameters, refit='AUC')
# gridsearch.fit(X_train, y_train)

# parameters = {'criterion' : ['gini'],
#               'bootstrap': [True],
#               'max_depth': [5, 10, 50, 100],
#               'n_estimators': [400, 600, 800]}

# RF = RandomForestClassifier(random_state=seed)

# gridsearch = GridSearchCV(RF, parameters, refit='f1_macro')
# gridsearch.fit(X_train, y_train)

# gridsearch.best_params_

"""###Performance Evaluation - RF"""

RF_predictions = modelRF.predict(X_test)
hmap = sns.heatmap(metrics.confusion_matrix(y_test, RF_predictions) , cmap='Blues', xticklabels=classes, yticklabels=classes)

print(classification_report(y_test, RF_predictions, target_names=classes))

"""##MLP"""

mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)

mlp.score(X_test, y_test)

MLP_predictions = mlp.predict(X_test)
hmap = sns.heatmap(metrics.confusion_matrix(y_test, MLP_predictions) , cmap='Blues', xticklabels=classes, yticklabels=classes)

print(classification_report(y_test, MLP_predictions, target_names=classes))